{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exp2_lstm_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisarahamedk/Kaggle_Notebooks/blob/master/notebooks/exp2_lstm_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XMa7c4DQQRKJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM Training on Raw Crypto Data"
      ]
    },
    {
      "metadata": {
        "id": "a4v28ivQQg_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1475
        },
        "outputId": "32f90040-9e98-4068-c5ce-789aca903266"
      },
      "cell_type": "code",
      "source": [
        "## Setting up fastai and PyTorch\n",
        "!curl https://course-v3.fast.ai/setup/colab | bash"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   665  100   665    0     0   3228      0 --:--:-- --:--:-- --:--:--  3228\n",
            "Collecting pillow==4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/e5/88b3d60924a3f8476fa74ec086f5fbaba56dd6cee0d82845f883b6b6dd18/Pillow-4.1.1-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.7MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow==4.1.1) (0.46)\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 5.4.0\n",
            "    Uninstalling Pillow-5.4.0:\n",
            "      Successfully uninstalled Pillow-5.4.0\n",
            "Successfully installed pillow-4.1.1\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
            "Collecting torch_nightly\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.0.0.dev20181206-cp36-cp36m-linux_x86_64.whl (576.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 576.2MB 23kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x61fe2000 @  0x7faf14e742a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch-nightly\n",
            "Successfully installed torch-nightly-1.0.0.dev20181206\n",
            "ln: failed to create symbolic link '/content/data': File exists\n",
            "Cloning into 'course-v3'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 2809 (delta 18), reused 33 (delta 15), pack-reused 2763\u001b[K\n",
            "Receiving objects: 100% (2809/2809), 105.13 MiB | 28.77 MiB/s, done.\n",
            "Resolving deltas: 100% (1530/1530), done.\n",
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/17/b5ab3f9a44b45661d9eb6e29001fe39c39ebb287ce9a9f1670d07bda0d4d/fastai-1.0.39-py3-none-any.whl (150kB)\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.1.1)\n",
            "Collecting bottleneck (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ae/cedf5323f398ab4e4ff92d6c431a3e1c6a186f9b41ab3e8258dff786a290/Bottleneck-1.2.1.tar.gz (105kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.0.18)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (18.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from fastai) (1.0.0)\n",
            "Collecting typing (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.2)\n",
            "Collecting nvidia-ml-py3 (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/64/cce82bddb80c0b0f5c703bbdafa94bfb69a1c5ad7a79cff00b482468f0d3/nvidia-ml-py3-7.352.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.9)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.22.0)\n",
            "Collecting dataclasses; python_version < \"3.7\" (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.14.6)\n",
            "Collecting fastprogress>=0.1.18 (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/78/57/24a5e20f4a357f7f1c90dd5250071951c832b2480fd4fefd7be48edf4180/fastprogress-0.1.18-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision->fastai) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2018.1.10)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (6.12.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2018.7)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0)\n",
            "Building wheels for collected packages: bottleneck, nvidia-ml-py3\n",
            "  Running setup.py bdist_wheel for bottleneck ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f2/bf/ec/e0f39aa27001525ad455139ee57ec7d0776fe074dfd78c97e4\n",
            "  Running setup.py bdist_wheel for nvidia-ml-py3 ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e4/1d/06/640c93f5270d67d0247f30be91f232700d19023f9e66d735c7\n",
            "Successfully built bottleneck nvidia-ml-py3\n",
            "Installing collected packages: bottleneck, typing, nvidia-ml-py3, dataclasses, fastprogress, fastai\n",
            "Successfully installed bottleneck-1.2.1 dataclasses-0.6 fastai-1.0.39 fastprogress-0.1.18 nvidia-ml-py3-7.352.0 typing-3.6.6\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c06RQrIoQ8zF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "1481c8c3-f5cf-4ec5-9c62-377efe3993d5"
      },
      "cell_type": "code",
      "source": [
        "## Creating directories to hold the data\n",
        "!mkdir -p data/crypto"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:74: RuntimeWarning: The _imaging extension was built for another version of Pillow or PIL\n",
            "  warnings.warn(str(v), RuntimeWarning)\n",
            "[autoreload of PIL.Image failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: The _imaging extension was built for another version of Pillow or PIL\n",
            "]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BHR-0tZtR29f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Upload the data using colab uploaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vENLdArjS-Ir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd data/crypto && unzip btc_baseline.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hfKQt9ugSadM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hUCkmoFgSgvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from fastai.tabular import *\n",
        "from fastai.text import *\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r77ZybBGSVrM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Load the data and Preprocess"
      ]
    },
    {
      "metadata": {
        "id": "GZbMrZAzSYvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = Path('.')\n",
        "data_path = path/'data/crypto'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9htYsG_US0Qa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "0f0583fd-7511-4cd9-daa4-ef092f06ead2"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_path/'btc_baseline.csv')\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>amount</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1542301200363</td>\n",
              "      <td>0.006175</td>\n",
              "      <td>5684.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1542301200376</td>\n",
              "      <td>0.005991</td>\n",
              "      <td>5684.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1542301200672</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>5683.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1542301200710</td>\n",
              "      <td>0.264759</td>\n",
              "      <td>5685.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1542301200710</td>\n",
              "      <td>0.135241</td>\n",
              "      <td>5683.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       timestamp    amount   price\n",
              "0  1542301200363  0.006175  5684.0\n",
              "1  1542301200376  0.005991  5684.0\n",
              "2  1542301200672  0.002000  5683.9\n",
              "3  1542301200710  0.264759  5685.4\n",
              "4  1542301200710  0.135241  5683.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "OZyAL5YwS1Oi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to comeup with a target variable which will be used for training. That is whether it is *buy, sell or hold*"
      ]
    },
    {
      "metadata": {
        "id": "qOmib-l-VOX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Target based on 1000 timesteps ahead price**"
      ]
    },
    {
      "metadata": {
        "id": "dH5BOc4zVe4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def raw_data_for_rnn_regression(df, label_t=1000, cls_thresh=0.02):\n",
        "    \"\"\"\n",
        "    function that adds a target variable, BUY, SELL or HOLD. based on the timestep it should look into future and \n",
        "    the threshold of price pct_change\n",
        "    :param df: \n",
        "    :param label_t: \n",
        "    :param cls_thresh: \n",
        "    :return: \n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(df)\n",
        "    # remove all columns except timestamp, amount, price\n",
        "    columns = ['timestamp', 'amount', 'price']\n",
        "    for col in df.columns:\n",
        "        if col not in columns:\n",
        "            df.drop(col, axis=1, inplace=True)\n",
        "\n",
        "    # set index timestamp\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    # Adding labels \n",
        "\n",
        "    df['target'] = df['price'].shift(-1)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoOYcRZ3W92h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "241dfd1f-47c4-42fa-ef8e-9f3e945ff346"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_path/'btc_baseline.csv')\n",
        "df.head()\n",
        "\n",
        "main_df = raw_data_for_rnn_regression(df)\n",
        "main_df.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amount</th>\n",
              "      <th>price</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1542301200363</th>\n",
              "      <td>0.006175</td>\n",
              "      <td>5684.0</td>\n",
              "      <td>5684.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542301200376</th>\n",
              "      <td>0.005991</td>\n",
              "      <td>5684.0</td>\n",
              "      <td>5683.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542301200672</th>\n",
              "      <td>0.002000</td>\n",
              "      <td>5683.9</td>\n",
              "      <td>5685.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542301200710</th>\n",
              "      <td>0.264759</td>\n",
              "      <td>5685.4</td>\n",
              "      <td>5683.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542301200710</th>\n",
              "      <td>0.135241</td>\n",
              "      <td>5683.9</td>\n",
              "      <td>5683.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 amount   price  target\n",
              "timestamp                              \n",
              "1542301200363  0.006175  5684.0  5684.0\n",
              "1542301200376  0.005991  5684.0  5683.9\n",
              "1542301200672  0.002000  5683.9  5685.4\n",
              "1542301200710  0.264759  5685.4  5683.9\n",
              "1542301200710  0.135241  5683.9  5683.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "ukF4ciimfdcw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "f0b58c4f-1b14-4048-af63-2d394eec9348"
      },
      "cell_type": "code",
      "source": [
        "main_df.reset_index(inplace=True)\n",
        "main_df.drop('timestamp', axis=1, inplace=True)\n",
        "main_df.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amount</th>\n",
              "      <th>price</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006175</td>\n",
              "      <td>5684.0</td>\n",
              "      <td>5684.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005991</td>\n",
              "      <td>5684.0</td>\n",
              "      <td>5683.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002000</td>\n",
              "      <td>5683.9</td>\n",
              "      <td>5685.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.264759</td>\n",
              "      <td>5685.4</td>\n",
              "      <td>5683.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.135241</td>\n",
              "      <td>5683.9</td>\n",
              "      <td>5683.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     amount   price  target\n",
              "0  0.006175  5684.0  5684.0\n",
              "1  0.005991  5684.0  5683.9\n",
              "2  0.002000  5683.9  5685.4\n",
              "3  0.264759  5685.4  5683.9\n",
              "4  0.135241  5683.9  5683.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "_Bv3867IoUSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating Train and Valid Set** "
      ]
    },
    {
      "metadata": {
        "id": "lC3V8g_moZI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "times = sorted(main_df.index.values)  # get the times\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
        "\n",
        "valid_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
        "train_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nOPocxuFo0tL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9b690938-304f-447f-fcd7-85fddd5a7a03"
      },
      "cell_type": "code",
      "source": [
        "print(len(train_df))\n",
        "print(len(valid_df))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3232569\n",
            "170135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uff3_E0KE7sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "cc2d83b6-b5e0-48e0-8426-8fafb58ca601"
      },
      "cell_type": "code",
      "source": [
        "# Normalize train and valid set\n",
        "cat_names = []\n",
        "cont_names = ['amount', 'price', 'target']\n",
        "tfm = Normalize(cat_names, cont_names)\n",
        "tfm(train_df)\n",
        "tfm(valid_df, test=True)\n",
        "\n",
        "train_df.head(), valid_df.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.obj[item_labels[indexer[info_axis]]] = value\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     amount     price    target\n",
              " 0  0.013168  2.960663  2.960668\n",
              " 1  0.013104  2.960663  2.960484\n",
              " 2  0.011723  2.960478  2.963255\n",
              " 3  0.102642  2.963250  2.960484\n",
              " 4  0.057827  2.960478  2.960299,            amount     price    target\n",
              " 3232569 -0.083262 -0.401553 -0.401552\n",
              " 3232570  0.008059 -0.401553 -0.401552\n",
              " 3232571 -1.156718 -0.401553 -0.403585\n",
              " 3232572  0.009301 -0.403585 -0.407280\n",
              " 3232573  0.004235 -0.407281 -0.415041)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "tbi54wJnNlEE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "d2718a77-2c77-43be-b9b9-7262a2148fc7"
      },
      "cell_type": "code",
      "source": [
        "full_df = pd.concat([train_df, valid_df])\n",
        "full_df.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amount</th>\n",
              "      <th>price</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.013168</td>\n",
              "      <td>2.960663</td>\n",
              "      <td>2.960668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.013104</td>\n",
              "      <td>2.960663</td>\n",
              "      <td>2.960484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.011723</td>\n",
              "      <td>2.960478</td>\n",
              "      <td>2.963255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.102642</td>\n",
              "      <td>2.963250</td>\n",
              "      <td>2.960484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.057827</td>\n",
              "      <td>2.960478</td>\n",
              "      <td>2.960299</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     amount     price    target\n",
              "0  0.013168  2.960663  2.960668\n",
              "1  0.013104  2.960663  2.960484\n",
              "2  0.011723  2.960478  2.963255\n",
              "3  0.102642  2.963250  2.960484\n",
              "4  0.057827  2.960478  2.960299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "josu9hyZo5_w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Pre-process the data**"
      ]
    },
    {
      "metadata": {
        "id": "aSl63nK6qE3x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(arr_x, arr_y, batch_size, seq_length):\n",
        "  \n",
        "  total_vals_in_a_batch = batch_size * seq_length\n",
        "  \n",
        "  num_batches = len(arr_x) // total_vals_in_a_batch\n",
        "  # print(f'Number of batches: {num_batches}')\n",
        "  \n",
        "  # keep only enough examples to form full batchs\n",
        "  arr_x = arr_x[:num_batches * total_vals_in_a_batch, :]\n",
        "  arr_y = arr_y[:num_batches * total_vals_in_a_batch, :]\n",
        "  \n",
        "  # reshape to batch_size x (seq_length * num_batches)\n",
        "  # so that every batch we can slide over and get next set of sequence in order\n",
        "  arr_x = arr_x.reshape(batch_size, -1, 2)\n",
        "  # print(f'arr_x shape: {arr_x.shape}')\n",
        "  arr_y = arr_y.reshape(batch_size, -1)\n",
        "  # print(f'arr_y_shape: {arr_y.shape}')\n",
        "  \n",
        "  # iterate through the array 1 sequence length at a time\n",
        "  for n in range(0, arr_x.shape[1], seq_length):\n",
        "    X = arr_x[:, n : n + seq_length]\n",
        "    y = arr_y[:, n : n + seq_length]\n",
        "    \n",
        "    yield X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TJQLHpXk6rgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "54b467de-40bb-4e7d-a33f-6485b6cebc59"
      },
      "cell_type": "code",
      "source": [
        "train_arr_x = train_df.values[:, 0: 2]\n",
        "train_arr_x.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3232569, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "cBjsEY6G7JIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3ff3e48f-0820-44e0-e5db-cfe40740d775"
      },
      "cell_type": "code",
      "source": [
        "train_arr_y = train_df.values[:, 2]\n",
        "train_arr_y.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3232569,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "8brIhnCeOUN7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "full_arr_x = full_df.values[:, 0:2]\n",
        "full_arr_x = full_arr_x.astype('float32')\n",
        "full_arr_y = full_df.values[:, 2]\n",
        "full_arr_y = full_arr_y.astype('float32')\n",
        "full_arr_y = np.expand_dims(full_arr_y, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFFK0GvX7T1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1522
        },
        "outputId": "dcbcc16e-50e8-4094-e35b-5cefffc76407"
      },
      "cell_type": "code",
      "source": [
        "batches = get_batches(full_arr_x, full_arr_y, batch_size=60, seq_length=1000)\n",
        "next(batches)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[ 1.316758e-02,  2.960663e+00],\n",
              "         [ 1.310357e-02,  2.960663e+00],\n",
              "         [ 1.172278e-02,  2.960478e+00],\n",
              "         [ 1.026424e-01,  2.963250e+00],\n",
              "         ...,\n",
              "         [-1.963008e-01,  3.069128e+00],\n",
              "         [-2.354139e-02,  3.069312e+00],\n",
              "         [-5.808556e-02,  3.067280e+00],\n",
              "         [-2.329357e-02,  3.062291e+00]],\n",
              " \n",
              "        [[ 1.204845e-02,  2.868274e+00],\n",
              "         [ 4.563267e-02,  2.868274e+00],\n",
              "         [ 4.563267e-02,  2.868274e+00],\n",
              "         [ 4.563267e-02,  2.868274e+00],\n",
              "         ...,\n",
              "         [-1.619789e-01,  2.820232e+00],\n",
              "         [ 1.424349e-02,  2.820417e+00],\n",
              "         [ 1.229452e-02,  2.820417e+00],\n",
              "         [ 1.004445e-02,  2.820232e+00]],\n",
              " \n",
              "        [[-6.270216e-03,  2.871970e+00],\n",
              "         [-6.270216e-03,  2.871970e+00],\n",
              "         [-6.270216e-03,  2.871970e+00],\n",
              "         [-6.270216e-03,  2.871970e+00],\n",
              "         ...,\n",
              "         [-4.210087e-03,  2.790852e+00],\n",
              "         [ 8.426874e-03,  2.790852e+00],\n",
              "         [ 1.102354e-02,  2.790852e+00],\n",
              "         [-1.079928e-03,  2.790298e+00]],\n",
              " \n",
              "        [[ 2.808472e-02,  2.301930e+00],\n",
              "         [ 2.356389e-03,  2.296942e+00],\n",
              "         [-1.027027e+00,  2.299528e+00],\n",
              "         [-2.552480e-02,  2.299159e+00],\n",
              "         ...,\n",
              "         [ 6.564779e-03,  2.291952e+00],\n",
              "         [ 9.096118e-02,  2.291971e+00],\n",
              "         [-9.893764e-03,  2.291952e+00],\n",
              "         [-4.282682e-03,  2.291952e+00]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-5.817310e-02, -3.641353e-01],\n",
              "         [ 1.129128e-03, -3.644124e-01],\n",
              "         [-3.326577e-01, -3.647820e-01],\n",
              "         [-5.284545e-03, -3.644124e-01],\n",
              "         ...,\n",
              "         [ 1.337946e-02, -4.161502e-01],\n",
              "         [ 4.769285e-03, -4.185523e-01],\n",
              "         [ 7.953960e-03, -4.185523e-01],\n",
              "         [ 1.895458e-02, -4.183675e-01]],\n",
              " \n",
              "        [[ 2.459034e-02, -6.737304e-01],\n",
              "         [ 5.204510e-02, -6.737304e-01],\n",
              "         [ 1.449094e-02, -6.737304e-01],\n",
              "         [ 3.423590e-02, -6.737304e-01],\n",
              "         ...,\n",
              "         [ 7.688123e-03, -6.696653e-01],\n",
              "         [ 1.494384e-01, -6.680022e-01],\n",
              "         [ 1.840404e-01, -6.681870e-01],\n",
              "         [ 4.841015e-03, -6.694805e-01]],\n",
              " \n",
              "        [[-7.432911e-01, -1.611569e-01],\n",
              "         [-1.619789e-01, -1.611569e-01],\n",
              "         [ 1.148365e-01, -1.694719e-01],\n",
              "         [ 2.835662e-02, -1.694719e-01],\n",
              "         ...,\n",
              "         [ 9.403972e-02, -1.825911e-01],\n",
              "         [ 7.570553e-03, -1.827759e-01],\n",
              "         [-1.106279e-01, -1.824433e-01],\n",
              "         [-9.684593e-02, -1.827759e-01]],\n",
              " \n",
              "        [[ 1.632792e-01, -3.310593e-02],\n",
              "         [ 1.840404e-01, -3.347548e-02],\n",
              "         [ 3.179190e-02, -3.366026e-02],\n",
              "         [-3.605868e-03, -3.369722e-02],\n",
              "         ...,\n",
              "         [ 3.179190e-02, -5.583359e-02],\n",
              "         [ 1.789687e-01, -5.583359e-02],\n",
              "         [ 1.449094e-02, -5.509448e-02],\n",
              "         [ 1.473685e-02, -5.287715e-02]]], dtype=float32),\n",
              " array([[ 2.960668,  2.960484,  2.963255,  2.960484, ...,  3.069318,  3.067285,  3.062296,  3.062296],\n",
              "        [ 2.868279,  2.868279,  2.868279,  2.868279, ...,  2.820422,  2.820422,  2.820237,  2.820422],\n",
              "        [ 2.871975,  2.871975,  2.871975,  2.871975, ...,  2.790857,  2.790857,  2.790303,  2.78864 ],\n",
              "        [ 2.296946,  2.299533,  2.299163,  2.299902, ...,  2.291975,  2.291957,  2.291957,  2.291957],\n",
              "        ...,\n",
              "        [-0.364412, -0.364781, -0.364412, -0.365521, ..., -0.418552, -0.418552, -0.418367, -0.418367],\n",
              "        [-0.67373 , -0.67373 , -0.67373 , -0.673915, ..., -0.668002, -0.668187, -0.66948 , -0.672622],\n",
              "        [-0.161156, -0.169471, -0.169471, -0.169471, ..., -0.182775, -0.182443, -0.182775, -0.18259 ],\n",
              "        [-0.033474, -0.033659, -0.033696, -0.033659, ..., -0.055833, -0.055094, -0.052876, -0.054354]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "-fTvMsULC_IP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define the Network"
      ]
    },
    {
      "metadata": {
        "id": "U3Jc6MuQ7hFf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "574aef19-7c16-46f0-aab3-c0e191adc9b2"
      },
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "  print('GPU Available')\n",
        "else:\n",
        "  print('No GPU, Will be training on CPU')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f6IdUtYm8XGl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Definition**"
      ]
    },
    {
      "metadata": {
        "id": "OXwlu7MB-N5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VanillaLSTM(nn.Module):\n",
        "  \n",
        "  def __init__(self, n_features, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.n_features = n_features\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "    \n",
        "    # the LSTM layer\n",
        "    self.lstm = nn.LSTM(input_size=n_features, hidden_size=self.n_hidden, num_layers=self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
        "    # Additional Dropout\n",
        "    self.dropout = nn.Dropout(self.drop_prob)\n",
        "    # Final FC layer, One hot encoded classes, BUY, SELL, HOLD\n",
        "    self.fc = nn.Linear(n_hidden, 1)\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    \n",
        "    # Get the output and new hidden state form LSTM\n",
        "    ### Debug Prints\n",
        "#     print(f'x on {x.device}')\n",
        "#     print(f'hidden on {hidden[0].device}')\n",
        "#     print(f'x shape {x.shape}')\n",
        "#     print(f'hidden shape {hidden[0].shape}')\n",
        "#     print(f'X type{x.type()}')\n",
        "#     print(f'hidden type{hidden[0].type()}')\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "    \n",
        "    # pass through a dropout layer\n",
        "    out = self.dropout(r_output[-1])\n",
        "    \n",
        "    # LSTM output will have shape batch_size x seq_length x n_hidden\n",
        "    # we need to stack em up to pass it to the fully connected\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    \n",
        "    # pass through the FC\n",
        "    out = self.fc(out)\n",
        "    \n",
        "    # return the final output and hidden state\n",
        "    return out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    \"\"\"\n",
        "    initial hidden state\n",
        "    \"\"\"\n",
        "    \n",
        "    weight = next(self.parameters()).data\n",
        "    \n",
        "    # the tuple represents the Long term and short term memory state of th LSTM\n",
        "    if(train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(), \n",
        "               weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(), \n",
        "               weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    return hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zE79hBUJu2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, data_x, data_y, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "  \n",
        "  net.train() # puttin the model in training mode.\n",
        "  \n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  criterion = nn.MSELoss()\n",
        "  \n",
        "  # create training and validation set\n",
        "  val_idx = int(len(data_x) * (1 - val_frac))\n",
        "  data_x, val_data_x = data_x[:val_idx], data_x[val_idx:]\n",
        "  data_y, val_data_y = data_y[:val_idx], data_y[val_idx:]\n",
        "  \n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "    \n",
        "  counter = 0\n",
        "  n_features = net.n_features\n",
        "  \n",
        "  for e in range(epochs):\n",
        "    \n",
        "    # initialize the hidden state of the network.\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    for x, y, in get_batches(data_x, data_y, batch_size, seq_length):\n",
        "      \n",
        "      counter += 1\n",
        "      \n",
        "      # convert to torch tensors\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      # put it on to GPU if available\n",
        "      if train_on_gpu:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      \n",
        "      # creating a new variable for the hidden state\n",
        "      # otherwise, we'd backprop through the entire history\n",
        "      h = ([each.data for each in h])\n",
        "      \n",
        "      # Zero accumulated gradients\n",
        "      net.zero_grad()\n",
        "      \n",
        "      # get the output of the model\n",
        "      outputs, h = net(inputs, h)\n",
        "      \n",
        "      ### Debug Prints\n",
        "#       print(f'outputs shape: {outputs.shape}')\n",
        "#       print(f'outputs type: {outputs.dtype}')\n",
        "#       print(outputs)\n",
        "#       print(f'targets shape: {targets.shape}')\n",
        "#       print(f'targets type: {targets.dtype}')\n",
        "#       print(targets)\n",
        "      \n",
        "      \n",
        "      # calculate the loss and perform backprop\n",
        "      loss = criterion(outputs, targets.view(batch_size * seq_length))\n",
        "      \n",
        "      loss.backward()\n",
        "      \n",
        "      # clip the gradients to avoid explosion\n",
        "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "      \n",
        "      opt.step()\n",
        "      \n",
        "      # print stats\n",
        "      if counter % print_every == 0:\n",
        "        # print(f'Validation, epoch: {counter}')\n",
        "        # Get validation loss\n",
        "        val_h = net.init_hidden(batch_size)\n",
        "        \n",
        "        val_losses = []\n",
        "        net.eval()\n",
        "        \n",
        "        for x, y in get_batches(val_data_x, val_data_y, batch_size, seq_length):\n",
        "          \n",
        "          # convert to torch tensors\n",
        "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "          \n",
        "          # creating new variable for the hidden state\n",
        "          val_h = ([each.data for each in val_h])\n",
        "          \n",
        "          inputs, targets = x, y\n",
        "          if train_on_gpu:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "          \n",
        "          output, val_h = net(inputs, val_h)\n",
        "          \n",
        "          val_loss = criterion(output, targets.view(batch_size * seq_length))\n",
        "          \n",
        "          val_losses.append(val_loss.item())\n",
        "          \n",
        "          # print(f'Validation, epoch: {counter}')\n",
        "          \n",
        "        net.train()\n",
        "        \n",
        "        print(f'Training Loss: {loss.item()}')\n",
        "        print(f'Validation Loss: {sum(val_losses) / len(val_losses)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HEbw-hB8KHk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Instantiating the model and Training"
      ]
    },
    {
      "metadata": {
        "id": "ARxYjPQ_MB1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "69f2cc75-f653-4e7e-aedb-94fe1b6bf293"
      },
      "cell_type": "code",
      "source": [
        "n_hidden = 1000\n",
        "n_layers = 2\n",
        "\n",
        "net = VanillaLSTM(n_features=2, n_hidden=n_hidden, n_layers=n_layers, drop_prob=0.2, lr=0.01)\n",
        "net"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VanillaLSTM(\n",
              "  (lstm): LSTM(2, 1000, num_layers=2, batch_first=True, dropout=0.2)\n",
              "  (dropout): Dropout(p=0.2)\n",
              "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "c1OwHxy5MWxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 30\n",
        "seq_length = 60\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbCxV6fOXXwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1185
        },
        "outputId": "a1ee6509-9b18-4c2b-da45-dbd61f8f6877"
      },
      "cell_type": "code",
      "source": [
        "train(net, full_arr_x, full_arr_y, epochs=epochs, batch_size=batch_size, seq_length=seq_length, lr=0.01, print_every=10)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 2.321678400039673\n",
            "Validation Loss: 0.03383177913055218\n",
            "Training Loss: 1.5044082403182983\n",
            "Validation Loss: 1.8738104054536768\n",
            "Training Loss: 2.1106483936309814\n",
            "Validation Loss: 0.6521342163363462\n",
            "Training Loss: 1.5973299741744995\n",
            "Validation Loss: 0.2744826959869849\n",
            "Training Loss: 1.920238733291626\n",
            "Validation Loss: 0.04779738422345232\n",
            "Training Loss: 1.6198408603668213\n",
            "Validation Loss: 1.9725683435561165\n",
            "Training Loss: 2.0517654418945312\n",
            "Validation Loss: 0.6531345282912885\n",
            "Training Loss: 1.5429884195327759\n",
            "Validation Loss: 0.23094485266498788\n",
            "Training Loss: 1.8946977853775024\n",
            "Validation Loss: 0.04785558786341753\n",
            "Training Loss: 1.6289088726043701\n",
            "Validation Loss: 1.8917193242481776\n",
            "Training Loss: 1.9327667951583862\n",
            "Validation Loss: 0.6243925454124571\n",
            "Training Loss: 1.4409536123275757\n",
            "Validation Loss: 0.23430687433512754\n",
            "Training Loss: 1.8136335611343384\n",
            "Validation Loss: 0.04652392876053613\n",
            "Training Loss: 1.568530559539795\n",
            "Validation Loss: 1.8664488186911932\n",
            "Training Loss: 1.9494311809539795\n",
            "Validation Loss: 0.6207380077195546\n",
            "Training Loss: 1.4617993831634521\n",
            "Validation Loss: 0.23111260370918052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-afe3f8fd5eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_arr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_arr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-5dd929652b5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_x, data_y, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     85\u001b[0m           \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m           \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Ehd9KIADcCDj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDC4GZ1uMgC8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}