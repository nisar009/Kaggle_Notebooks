{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Training on TPU"},{"metadata":{},"cell_type":"markdown","source":"### Install packages"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# %%capture\n!pip install -U efficientnet","execution_count":20,"outputs":[{"output_type":"stream","text":"Requirement already up-to-date: efficientnet in /opt/conda/lib/python3.7/site-packages (1.1.0)\nRequirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet) (1.0.8)\nRequirement already satisfied, skipping upgrade: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet) (0.16.2)\nRequirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.18.1)\nRequirement already satisfied, skipping upgrade: scipy>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (1.4.1)\nRequirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (3.2.1)\nRequirement already satisfied, skipping upgrade: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (2.4)\nRequirement already satisfied, skipping upgrade: pillow>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (5.4.1)\nRequirement already satisfied, skipping upgrade: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (2.8.0)\nRequirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (1.1.1)\nRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.14.0)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.7)\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.2.0)\nRequirement already satisfied, skipping upgrade: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting tensorflow==2.1\n  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n\u001b[K     |████████████████████████████████| 421.8 MB 15 kB/s s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.14.0)\nCollecting tensorboard<2.2.0,>=2.1.0\n  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 54.4 MB/s eta 0:00:01\n\u001b[?25hCollecting gast==0.2.2\n  Downloading gast-0.2.2.tar.gz (10 kB)\nCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n\u001b[K     |████████████████████████████████| 448 kB 39.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.18.1)\nRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.11.2)\nRequirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (0.34.2)\nRequirement already satisfied, skipping upgrade: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (0.8.1)\nRequirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (3.11.4)\nRequirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.0.8)\nRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (3.2.1)\nRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (0.9.0)\nRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.1.0)\nRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.28.1)\nRequirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.4.1)\nRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (1.1.0)\nRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1) (0.2.0)\nRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.2.1)\nRequirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (46.1.3.post20200325)\nRequirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.0.1)\nRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.23.0)\nRequirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.14.0)\nRequirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.1)\nRequirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1) (2.10.0)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2020.4.5.1)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.9)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.24.3)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\nRequirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.2.7)\nRequirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.1)\nRequirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0)\nRequirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.2.0)\nRequirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.8)\nRequirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.1)\nBuilding wheels for collected packages: gast\n","name":"stdout"},{"output_type":"stream","text":"  Building wheel for gast (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=3ffebd6bdbd32ef2070e70552089e805540a4389f8304fba0c3c297814003ead\n  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\nSuccessfully built gast\nInstalling collected packages: tensorboard, gast, tensorflow-estimator, tensorflow\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.2.2\n    Uninstalling tensorboard-2.2.2:\n      Successfully uninstalled tensorboard-2.2.2\n  Attempting uninstall: gast\n    Found existing installation: gast 0.3.3\n    Uninstalling gast-0.3.3:\n      Successfully uninstalled gast-0.3.3\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.2.0\n    Uninstalling tensorflow-estimator-2.2.0:\n      Successfully uninstalled tensorflow-estimator-2.2.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.2.0\n    Uninstalling tensorflow-2.2.0:\n      Successfully uninstalled tensorflow-2.2.0\nSuccessfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom functools import partial\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.backend as K\n\nimport pandas as pd\nimport numpy as np\nimport wandb\nfrom wandb.keras import WandbCallback\nfrom sklearn import model_selection\n\nimport efficientnet.tfkeras as efn\nfrom kaggle_datasets import KaggleDatasets","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WandB Login"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wandb login f137298421da563b24639d1287dd3ce5da537814","execution_count":25,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"notes = \"one cycle learning rate\"\nwandb.init(project=\"kaggle-melanoma\", notes=notes)","execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/nisarahamedk/kaggle-melanoma\" target=\"_blank\">https://app.wandb.ai/nisarahamedk/kaggle-melanoma</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/nisarahamedk/kaggle-melanoma/runs/2g3bjexo\" target=\"_blank\">https://app.wandb.ai/nisarahamedk/kaggle-melanoma/runs/2g3bjexo</a><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.8.36 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"W&B Run: https://app.wandb.ai/nisarahamedk/kaggle-melanoma/runs/2g3bjexo"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## TPU Config"},{"metadata":{},"cell_type":"markdown","source":"Detect hardware and return appropriate distribution strategy"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # no parameter needed for TPU_NAME env variable is set. This is the case for Kaggle\n    print(\"Running on TPU: \", tpu.master())\nexcept ValueError:\n    tpu = None","execution_count":27,"outputs":[{"output_type":"stream","text":"Running on TPU:  grpc://10.0.0.2:8470\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # default strategy with the available hw\n    strategy = tf.distribute.get_strategy()\n    \nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":28,"outputs":[{"output_type":"stream","text":"REPLICAS:  8\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Dataset from GCS for TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path(\"siim-isic-melanoma-classification\")","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"'gs://kds-c89313da1d85616eec461ab327fed61e1335defb486fb7729cf897b1'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!gsutil ls $GCS_DS_PATH # list the GCS bucket","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files = tf.io.gfile.glob(GCS_DS_PATH + \"/tfrecords/train*\")\ntest_files = tf.io.gfile.glob(GCS_DS_PATH + \"/tfrecords/test*\")","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_files), len(test_files)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"(16, 16)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Train, Valid split"},{"metadata":{"trusted":true},"cell_type":"code","source":"LOCAL_DS_PATH = Path(\"/kaggle/input/siim-isic-melanoma-classification\")","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(LOCAL_DS_PATH / \"train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assign a fold id for each images using StratifiedKFold"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"kfold\"] = -1\n\ny = train_df[\"target\"].values\n\nskf = model_selection.StratifiedKFold(n_splits=5, shuffle=True)\n\nfor fold, (train_idx, test_idx) in enumerate(skf.split(X=train_df, y=y)):\n    train_df.loc[test_idx, \"kfold\"] = fold\n    \ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This way, when we run training with fold=1, images with column \"kfold=1\" will be kept in validation set, others in training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"kfold\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### tf.Dataset pipeline "},{"metadata":{},"cell_type":"markdown","source":"Building the complete tfrecord -> model data pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = tf.data.TFRecordDataset(train_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)\ntest_dataset = tf.data.TFRecordDataset(test_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking the feature discription of the tfreceord files"},{"metadata":{},"cell_type":"markdown","source":"We have features: \"image\", \"image_name\" and \"target\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# test set\nfor item in test_dataset.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(item.numpy())\n    print(example)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have \"image\" and \"image_name\" for the test dataset"},{"metadata":{},"cell_type":"markdown","source":"#### Feature description\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature_desc = {\n    \"image\": tf.io.FixedLenFeature([], tf.string),\n    \"image_name\": tf.io.FixedLenFeature([], tf.string), # for filtering images belong to val set.\n    \"target\": tf.io.FixedLenFeature([], tf.int64),\n}\n\ntest_feature_desc = {\n    \"image\": tf.io.FixedLenFeature([], tf.string),\n    \"image_name\": tf.io.FixedLenFeature([], tf.string),\n}","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the above feature description, \n* Lets load the dataset from the tfrecord files\n* Process it using the feature description.\n* Decode each sample into an image.\n* return image, target pairs  \n\n*Read from bottom to top*"},{"metadata":{"trusted":true},"cell_type":"code","source":"TPU_IMAGE_SIZE = 1024\nINPUT_IMAGE_SIZE = 512","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold = train_df[[\"image_name\", \"kfold\"]].set_index(\"image_name\").to_dict()\nfold = fold[\"kfold\"]\nfold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    image_size = [TPU_IMAGE_SIZE, TPU_IMAGE_SIZE]\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.reshape(image, [*image_size, 3])  # explicit size needed for TPU\n    image = tf.image.resize(image, [INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE])\n    return image\n                           \ndef parse_test_example(example):\n    \"\"\" function to parse each example read from the test tfrecord file\"\"\"\n    example = tf.io.parse_single_example(example, test_feature_desc)\n    return example\n\ndef parse_train_example(example):\n    \"\"\" function to parse each example read from the train tfrecord file\"\"\"\n    example = tf.io.parse_single_example(example, train_feature_desc)\n    return example\n\ndef process_train_example(example):\n    image = decode_image(example[\"image\"])\n    label = tf.cast(example[\"target\"], tf.int32)\n    return image, label\n\ndef process_test_example(example):\n    image = decode_image(example[\"image\"])\n    image_name = example[\"image_name\"]\n    return image, image_name\n\ndef train_filter_fn(example):\n    # convert folds dict to tensorflow lookup table.\n    img_names = tf.constant(list(fold.keys()))\n    fold_idx = tf.constant(list(fold.values()))\n    folds_init = tf.lookup.KeyValueTensorInitializer(img_names, fold_idx)\n    folds_table = tf.lookup.StaticHashTable(folds_init, -1)\n    return folds_table.lookup(example[\"image_name\"]) != 1\n    \ndef valid_filter_fn(example):\n    # convert folds dict to tensorflow lookup table.\n    img_names = tf.constant(list(fold.keys()))\n    fold_idx = tf.constant(list(fold.values()))\n    folds_init = tf.lookup.KeyValueTensorInitializer(img_names, fold_idx)\n    folds_table = tf.lookup.StaticHashTable(folds_init, -1)\n    return folds_table.lookup(example[\"image_name\"]) == 1\n    \ndef load_dataset_from_tfrecord(filenames, ds_type=\"train\", ordered=False):\n    \n    # Since we are reading dataset from multiple files. and we dont care about the order.\n    # set deterministic reading to False.\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    dataset.with_options(ignore_order)\n    \n    # parse each example with feature description\n    if ds_type in [\"train\", \"valid\"]:\n        dataset = dataset.map(parse_train_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        # filter  \n        # dataset = dataset.filter(train_filter_fn if ds_type==\"train\" else valid_filter_fn)\n        dataset = dataset.map(process_train_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    else:\n        dataset = dataset.map(parse_test_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(process_test_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n    return dataset\n    ","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data augmentation for the image"},{"metadata":{"trusted":true},"cell_type":"code","source":"def zoom(x: tf.Tensor) -> tf.Tensor:\n    \"\"\"Zoom augmentation\n\n    Args:\n        x: Image\n\n    Returns:\n        Augmented image\n    \"\"\"\n\n    # Generate 20 crop settings, ranging from a 1% to 20% crop.\n    scales = list(np.arange(0.8, 1.0, 0.01))\n    boxes = np.zeros((len(scales), 4))\n\n    for i, scale in enumerate(scales):\n        x1 = y1 = 0.5 - (0.5 * scale)\n        x2 = y2 = 0.5 + (0.5 * scale)\n        boxes[i] = [x1, y1, x2, y2]\n\n    def random_crop(img):\n        # Create different crops for an image\n        crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=(32, 32))\n        # Return a random crop\n        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n\n\n    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n\n    # Only apply cropping 50% of the time\n    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augment(image, label):\n    # https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/\n    # try tfaddons augmentations\n    \n    # orientation\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    # color augmentation\n    image = tf.image.random_hue(image, 0.08)\n    image = tf.image.random_saturation(image, 0.6, 1.6)\n    image = tf.image.random_brightness(image, 0.05)\n    image = tf.image.random_contrast(image, 0.7, 1.3)\n    \n    # Random Zoom, This crops the image, need to be careful here, as the image shape changes\n    # image = zoom(image)\n    return image, label","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, the datapipeline function that puts these all together"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16 * strategy.num_replicas_in_sync\ndef get_training_dataset():\n    dataset = load_dataset_from_tfrecord(train_files[1:], ds_type=\"train\")\n    dataset = dataset.map(data_augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    # dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_validation_dataset():\n    dataset = load_dataset_from_tfrecord([train_files[0]], ds_type=\"valid\")\n    dataset = dataset.map(data_augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    # dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_dataset(ordered=False):\n    dataset = load_dataset_from_tfrecord(test_files, ds_type=\"test\", ordered=ordered)\n    dataset = dataset.batch(batch_size)\n    dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity check"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*** Training set shapes *****\")\nfor image, label in get_training_dataset().take(3):\n    print(image.numpy().shape, label.numpy().shape)\n    \nprint(\"*** Training set labels: \", label.numpy())\n\nprint(\"*** Validation set shapes *****\")\nfor image, label in get_validation_dataset().take(3):\n    print(image.numpy().shape, label.numpy().shape)\n    \nprint(\"*** Validation set labels: \", label.numpy())\n\n\nprint(\"*** Test set shape ***\")\nfor image, image_name in get_test_dataset().take(3):\n    print(image.numpy().shape, image_name.numpy().shape)\nprint(\"*** Test set image_name: \", image_name.numpy().astype(\"U\"))","execution_count":42,"outputs":[{"output_type":"stream","text":"*** Training set shapes *****\n(128, 512, 512, 3) (128,)\n(128, 512, 512, 3) (128,)\n(128, 512, 512, 3) (128,)\n*** Training set labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n*** Validation set shapes *****\n(128, 512, 512, 3) (128,)\n(128, 512, 512, 3) (128,)\n(128, 512, 512, 3) (128,)\n*** Validation set labels:  [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n*** Test set shape ***\n(128, 512, 512, 3) (128,)\n(128, 512, 512, 3) (128,)\n(128, 512, 512, 3) (128,)\n*** Test set image_name:  ['ISIC_3009035' 'ISIC_1579773' 'ISIC_6082685' 'ISIC_1263999'\n 'ISIC_4348477' 'ISIC_3740111' 'ISIC_1071664' 'ISIC_1560363'\n 'ISIC_5223297' 'ISIC_7851934' 'ISIC_7889564' 'ISIC_4556233'\n 'ISIC_4384149' 'ISIC_1673959' 'ISIC_4059973' 'ISIC_4624150'\n 'ISIC_5420567' 'ISIC_1921353' 'ISIC_3164346' 'ISIC_9655502'\n 'ISIC_8469607' 'ISIC_2598603' 'ISIC_4263498' 'ISIC_7894734'\n 'ISIC_7206758' 'ISIC_5966072' 'ISIC_3609057' 'ISIC_8941788'\n 'ISIC_5787712' 'ISIC_7743612' 'ISIC_5670844' 'ISIC_0142066'\n 'ISIC_9308692' 'ISIC_9782532' 'ISIC_8179996' 'ISIC_0082004'\n 'ISIC_4244166' 'ISIC_0809747' 'ISIC_8472353' 'ISIC_6759399'\n 'ISIC_5294076' 'ISIC_4778220' 'ISIC_0473489' 'ISIC_3379782'\n 'ISIC_7027005' 'ISIC_8188409' 'ISIC_0620581' 'ISIC_5731724'\n 'ISIC_0989319' 'ISIC_8437178' 'ISIC_2331559' 'ISIC_1675841'\n 'ISIC_0756977' 'ISIC_9825062' 'ISIC_7597450' 'ISIC_1494327'\n 'ISIC_2695905' 'ISIC_2361309' 'ISIC_2402484' 'ISIC_6885654'\n 'ISIC_9589360' 'ISIC_4808532' 'ISIC_3614842' 'ISIC_3937767'\n 'ISIC_0169575' 'ISIC_9497933' 'ISIC_0395885' 'ISIC_8678730'\n 'ISIC_6900567' 'ISIC_0620316' 'ISIC_7904453' 'ISIC_3414807'\n 'ISIC_3514405' 'ISIC_1354933' 'ISIC_2680409' 'ISIC_1373939'\n 'ISIC_7785749' 'ISIC_1889661' 'ISIC_5003515' 'ISIC_0433895'\n 'ISIC_5285042' 'ISIC_9253871' 'ISIC_7240170' 'ISIC_0685067'\n 'ISIC_1725083' 'ISIC_6677512' 'ISIC_1450044' 'ISIC_8664024'\n 'ISIC_7411679' 'ISIC_3096116' 'ISIC_8443990' 'ISIC_7114023'\n 'ISIC_6426773' 'ISIC_5944731' 'ISIC_6185319' 'ISIC_1904819'\n 'ISIC_9853298' 'ISIC_7671624' 'ISIC_3188729' 'ISIC_7014635'\n 'ISIC_9448856' 'ISIC_7736823' 'ISIC_0313675' 'ISIC_3086004'\n 'ISIC_7414621' 'ISIC_6438534' 'ISIC_2922450' 'ISIC_8122278'\n 'ISIC_1100351' 'ISIC_6244226' 'ISIC_9875614' 'ISIC_6755755'\n 'ISIC_6903816' 'ISIC_0372971' 'ISIC_4031425' 'ISIC_7654912'\n 'ISIC_1233031' 'ISIC_2722983' 'ISIC_2543386' 'ISIC_6683074'\n 'ISIC_9093252' 'ISIC_7753184' 'ISIC_2028661' 'ISIC_7106250'\n 'ISIC_4498984' 'ISIC_9737914' 'ISIC_2455226' 'ISIC_7468924']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Model - Efficient Net "},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = keras.Sequential([\n        efn.EfficientNetB7(\n            input_shape=[INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3],\n            weights=\"imagenet\",\n            include_top=False,\n        ),\n        keras.layers.GlobalAveragePooling2D(),\n        keras.layers.Dense(1024, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(512, activation=\"relu\"),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(128, activation=\"relu\"),\n        keras.layers.Dropout(0.1),\n        keras.layers.Dense(1, activation=\"sigmoid\"),\n    ])","execution_count":43,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n258441216/258434480 [==============================] - 18s 0us/step\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### One cycle LR Scheduler"},{"metadata":{"trusted":true},"cell_type":"code","source":"class OneCycleLRScheduler(keras.callbacks.Callback):\n    def __init__(\n        self,\n        iterations,\n        max_rate,\n        start_rate=None,\n        last_iterations=None,\n        last_rate=None,\n    ):\n        self.iterations = iterations\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or self.iterations / 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n\n#         logging.info(f\"Total iterations: {self.iterations}\")\n#         logging.info(f\"Max rate: {self.max_rate}\")\n#         logging.info(f\"start_rate: {self.start_rate}\")\n#         logging.info(f\"last_iterations: {self.last_iterations}\")\n#         logging.info(f\"half_iteration: {self.half_iteration}\")\n#         logging.info(f\"last_rate: {self.last_rate}\")\n\n        self.lrs = []\n        self.losses = []\n\n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return (rate2 - rate1) * (iter2 - self.iteration) / (iter2 - iter1) + rate1\n\n    def on_batch_begin(self, batch, logs):  # pylint: disable=unused-argument\n        if self.iteration < self.half_iteration:\n            rate = self._interpolate(\n                0, self.half_iteration, self.start_rate, self.max_rate\n            )\n        elif self.iteration < 2 * self.half_iteration:\n            rate = self._interpolate(\n                self.half_iteration,\n                2 * self.half_iteration,\n                self.max_rate,\n                self.start_rate,\n            )\n        else:\n            rate = self._interpolate(\n                2 * self.half_iteration,\n                self.iterations,\n                self.start_rate,\n                self.last_rate,\n            )\n            rate = max(rate, self.last_rate)\n\n        self.lrs.append(rate)\n        self.iteration += 1\n        K.set_value(self.model.optimizer.lr, rate)\n\n    def on_batch_end(self, batch, logs):  # pylint: disable=unused-argument\n        self.losses.append(logs[\"loss\"])\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs[\"lr\"] = K.get_value(self.model.optimizer.lr)\n\n\ndef find_lr(model, dataset, epochs=1, min_rate=1e-6, max_rate=1):\n    \"\"\"find a starting point LR for using as a max_rate for OneCycleLRScheduler\"\"\"\n    init_weights = model.get_weights()  # backup\n    steps = dataset.n_train_batches * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / steps)\n\n    init_lr = K.get_value(model.optimizer.lr)\n    K.set_value(model.optimizer.lr, min_rate)\n    exp_lr = ExponentialLRScheduler(factor)\n\n    history = model.fit(\n        dataset.train_ds,\n        epochs=epochs,\n        steps_per_epoch=dataset.n_train_batches,\n        callbacks=[exp_lr],\n    )\n\n    # restore\n    K.set_value(model.optimizer.lr, init_lr)\n    model.set_weights(init_weights)\n\n    return exp_lr.losses, exp_lr.lrs","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer=\"adam\", \n    # loss=\"binary_crossentropy\", \n    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.1), \n    metrics=[\"accuracy\", keras.metrics.AUC()]\n)\n\nmodel.summary()","execution_count":48,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nefficientnet-b7 (Model)      (None, 16, 16, 2560)      64097680  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2560)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1024)              2622464   \n_________________________________________________________________\ndropout (Dropout)            (None, 1024)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               524800    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               131328    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 67,409,297\nTrainable params: 67,098,577\nNon-trainable params: 310,720\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"onecycle_cb = OneCycleLRScheduler(iterations=250 * 10, max_rate=1e-2)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(get_training_dataset(), validation_data=get_validation_dataset(), epochs=10, callbacks=[WandbCallback(), onecycle_cb])","execution_count":50,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.8.36 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"stream","text":"Epoch 1/10\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'K' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     with training_context.on_batch(\n\u001b[0;32m--> 126\u001b[0;31m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0m\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    780\u001b[0m       self.callbacks._call_batch_hook(\n\u001b[0;32m--> 781\u001b[0;31m           mode, 'begin', step, batch_logs)\n\u001b[0m\u001b[1;32m    782\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;31m# For backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-8d69c3380025>\u001b[0m in \u001b[0;36mon_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'K' is not defined","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-a5777aca72b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_validation_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monecycle_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/keras/__init__.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-8d69c3380025>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True)\n\nprint(\"Computing predictions...\")\ntest_image_ds = test_ds.map(lambda image, image_name: image)\nprobs = model.predict(test_image_ds).flatten()\nprint(probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Generating submission file\")\nnum_test_images = count_data_items(test_files)\ntest_image_names_ds = test_ds.map(lambda image, image_name: image_name).unbatch()\n\ntest_image_names = next(iter(test_image_names_ds.batch(num_test_images))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_image_names, probs]), fmt=['%s', '%f'], delimiter=',', header='image_name,target', comments='')\n!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}